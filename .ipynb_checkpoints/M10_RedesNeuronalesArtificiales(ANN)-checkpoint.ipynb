{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4740728a-712b-47fe-a389-861e8c07768d",
   "metadata": {},
   "source": [
    "<H1>Marco Teórico: Redes Neuronales Artificiales (ANN) </H1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f51da4-537b-478c-b596-99fee2710657",
   "metadata": {},
   "source": [
    "#### **1. Introducción a Redes Neuronales Artificiales (ANN)**\n",
    "\n",
    "Las redes neuronales artificiales (ANN, por sus siglas en inglés) son modelos computacionales que imitan el funcionamiento del cerebro humano para realizar tareas como la clasificación, predicción y reconocimiento de patrones. Están compuestas por múltiples **capas** de **neuronas** conectadas entre sí, donde cada conexión tiene un **peso** y un **sesgo**, y los valores se propagan hacia adelante usando **funciones de activación**. El aprendizaje se realiza mediante el algoritmo de **retropropagación**.\n",
    "\n",
    "##### **1.1 Capas de una Red Neuronal**\n",
    "Una red neuronal típica consta de tres tipos de capas:\n",
    "- **Capa de Entrada:** Recibe las características de entrada $x_1, x_2, \\ldots, x_n $.\n",
    "- **Capas Ocultas:** Transforman las entradas en funciones intermedias.\n",
    "- **Capa de Salida:** Produce el resultado final $y$.\n",
    "\n",
    "##### **1.2 Cálculo del Peso, Sesgo y la Salida de una Neurona**\n",
    "\n",
    "Cada neurona en una red neuronal procesa la información de las neuronas de la capa anterior. Esto se realiza a través de una combinación lineal de las entradas, ponderadas por los **pesos** correspondientes, y sumando un **sesgo**. Posteriormente, esta combinación se pasa a través de una **función de activación** para obtener la **salida** de la neurona.\n",
    "\n",
    "La salida de una neurona $i$ en una capa $l$ se calcula mediante los siguientes pasos:\n",
    "\n",
    "1. **Combinación Lineal de las Entradas:**\n",
    "\n",
    "   \n",
    "   La combinación lineal de las entradas a una neurona se calcula como:\n",
    "\n",
    "   $\n",
    "   z_i^l = \\sum_{j=1}^{n} w_{ij}^l x_j^{l-1} + b_i^l\n",
    "   $\n",
    "\n",
    "   Donde:\n",
    "   - $w_{ij}^l$ es el peso de la conexión entre la neurona $j$ de la capa anterior $l-1$ y la neurona $i$ de la capa actual $l$.\n",
    "   - $b_i^l$ es el sesgo asociado a la neurona $i$ en la capa $l$.\n",
    "   - $x_j^{l-1}$ es la salida de la neurona $j$ en la capa anterior $l-1$.\n",
    "   - $z_i^l$ es el valor combinado antes de aplicar la función de activación.\n",
    "\n",
    "   El peso $w_{ij}^l$ es un valor que indica la **fuerza** o **importancia** de la conexión entre las neuronas $i$ y $j$, mientras que el sesgo $b_i^l$ ayuda a desplazar la salida de la combinación lineal y ajustarla para mejorar el aprendizaje.\n",
    "\n",
    "3. **Aplicación de la Función de Activación:**\n",
    "   \n",
    "   Para que la red neuronal pueda modelar relaciones no lineales, el valor $z_i^l$ se transforma a través de una **función de activación**. Las funciones de activación más comunes incluyen:\n",
    "\n",
    "   - **Sigmoide:**\n",
    "   \n",
    "   $\n",
    "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "   $\n",
    "\n",
    "   Esta función transforma el valor $z$ en un número entre 0 y 1, útil en modelos de clasificación binaria.\n",
    "\n",
    "   - **ReLU (Rectified Linear Unit):**\n",
    "   $\n",
    "   \\text{ReLU}(z) = \\max(0, z)\n",
    "   $\n",
    "   \n",
    "   Esta función convierte cualquier valor negativo en cero, permitiendo solo pasar valores positivos. Es muy utilizada en redes neuronales profundas debido a su simplicidad y eficiencia en el entrenamiento.\n",
    "\n",
    "   - **Tangente Hiperbólica (Tanh):**\n",
    "   $\n",
    "   \\text{tanh}(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\n",
    "   $\n",
    "   \n",
    "   La tangente hiperbólica transforma $z$ en un valor entre -1 y 1, lo que permite que las salidas estén centradas en torno a cero.\n",
    "\n",
    "5. **Salida Final de la Neurona:**\n",
    "   \n",
    "   La salida de la neurona $i$ en la capa $l$, denotada como $a_i^l$, es simplemente el resultado de aplicar la función de activación al valor combinado $z_i^l$:\n",
    "\n",
    "   $\n",
    "   a_i^l = f(z_i^l) = f\\left( \\sum_{j=1}^{n} w_{ij}^l x_j^{l-1} + b_i^l \\right)\n",
    "   $\n",
    "\n",
    "   Esta salida se convierte en la entrada para las neuronas de la siguiente capa o, en el caso de la capa de salida, corresponde a la predicción final del modelo.\n",
    "\n",
    "##### **1.3 Retropropagación**\n",
    "El algoritmo de retropropagación es el corazón del aprendizaje de una red neuronal, que ajusta los pesos y los sesgos minimizando una función de pérdida. La **función de pérdida** comúnmente usada es el **error cuadrático medio** para problemas de regresión o la **entropía cruzada** para problemas de clasificación.\n",
    "\n",
    "$\n",
    "\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $y_i$ es el valor real.\n",
    "- $\\hat{y}_i$ es el valor predicho.\n",
    "- $N$ es el número de ejemplos en el conjunto de entrenamiento.\n",
    "\n",
    "La actualización de los pesos se realiza mediante la **regla de gradiente descendente**:\n",
    "\n",
    "$\n",
    "w_{ij}^l \\leftarrow w_{ij}^l - \\eta \\frac{\\partial \\mathcal{L}}{\\partial w_{ij}^l}\n",
    "$\n",
    "\n",
    "Donde:\n",
    "- $\\eta$ es la tasa de aprendizaje.\n",
    "- $\\frac{\\partial \\mathcal{L}}{\\partial w_{ij}^l}$ es el gradiente de la pérdida respecto al peso $w_{ij}^l$.\n",
    "\n",
    "#### **2. Arquitectura de Redes Neuronales Profundas (DNN)**\n",
    "\n",
    "Las redes neuronales profundas (**DNN**) son una extensión de las redes neuronales que incluyen múltiples **capas ocultas**. Estas capas permiten aprender características jerárquicas de los datos, lo que hace que las DNN sean extremadamente poderosas en tareas complejas.\n",
    "\n",
    "##### **2.1 Regularización en Redes Neuronales Profundas**\n",
    "Para evitar el **sobreajuste** en redes neuronales profundas, se utilizan técnicas de regularización como:\n",
    "- **Regularización $L_2$:** Añade un término de penalización en la función de pérdida para controlar el tamaño de los pesos.\n",
    "$\n",
    "\\mathcal{L}_{L_2} = \\mathcal{L} + \\lambda \\sum_{i,j} (w_{ij}^2)\n",
    "$\n",
    "- **Dropout:** Durante el entrenamiento, se desactivan aleatoriamente neuronas para evitar que la red dependa excesivamente de ciertas neuronas.\n",
    "\n",
    "#### **3. Implementación de Redes Neuronales Artificiales con TensorFlow y Keras**\n",
    "\n",
    "**TensorFlow** es una biblioteca de cálculo numérico y machine learning, mientras que **Keras** es una API de alto nivel que facilita la construcción de redes neuronales.\n",
    "\n",
    "##### **3.1 Proceso de Entrenamiento**\n",
    "El proceso de entrenamiento de una ANN consiste en los siguientes pasos:\n",
    "1. Inicializar pesos y sesgos.\n",
    "2. Propagar las entradas hacia adelante para calcular las predicciones.\n",
    "3. Calcular la función de pérdida.\n",
    "4. Propagar hacia atrás el error para ajustar los pesos.\n",
    "5. Repetir el proceso hasta que se minimice la pérdida.\n",
    "\n",
    "##### **3.2 Métricas de Evaluación**\n",
    "Para evaluar el rendimiento de una red neuronal, se utilizan métricas como:\n",
    "- **Precisión (Accuracy):**\n",
    "$\n",
    "\\text{Accuracy} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{1}(\\hat{y}_i = y_i)\n",
    "$\n",
    "- **Precisión (Precision):**\n",
    "$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$\n",
    "- **Exhaustividad (Recall):**\n",
    "$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$\n",
    "- **F1-Score:**\n",
    "$\n",
    "F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$\n",
    "\n",
    "Donde:\n",
    "- $TP$ son los verdaderos positivos.\n",
    "- $FP$ son los falsos positivos.\n",
    "- $FN$ son los falsos negativos.\n",
    "\n",
    "##### **3.3 Ajuste de Hiperparámetros**\n",
    "El ajuste de hiperparámetros incluye:\n",
    "- **Número de neuronas** en cada capa.\n",
    "- **Número de capas ocultas.**\n",
    "- **Tasa de aprendizaje $\\eta$.**\n",
    "- **Número de épocas** de entrenamiento.\n",
    "\n",
    "El rendimiento final del modelo depende de encontrar los valores óptimos para estos hiperparámetros mediante técnicas como la **búsqueda en cuadrícula** o **optimización bayesiana**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfb8033-dbc2-420d-94aa-d9e0bb25a170",
   "metadata": {},
   "source": [
    "**Módulo 10: Redes Neuronales Artificiales (ANN)**\n",
    "\n",
    "**Conceptos clave:**\n",
    "\n",
    "Introducción a redes neuronales: capas, activaciones y retropropagación.\n",
    "\n",
    "Arquitectura de redes neuronales profundas.\n",
    "\n",
    "Implementación de ANN con TensorFlow y Keras.\n",
    "\n",
    "**Proyecto: Clasificación de imágenes con redes neuronales.**\n",
    "\n",
    "Utilizar el dataset CIFAR-10 o MNIST para entrenar una red neuronal que clasifique imágenes. Evaluar su rendimiento utilizando técnicas avanzadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edd91e8-1494-4b31-9496-19b3cd7d3b87",
   "metadata": {},
   "source": [
    "**1. Instalación de Librerías Necesarias**\n",
    "\n",
    "Si aún no tienes TensorFlow y Keras instalados, instálalos usando pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9939304e-e58e-4930-aa5d-234073cd4960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd26e62-ce96-471f-b2db-2be32480e946",
   "metadata": {},
   "source": [
    "**2. Importación de Datos**\n",
    "\n",
    "Vamos a usar el dataset MNIST, que está disponible en TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2642389e-b021-4b50-819c-df4946fe2fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 11:25:36.857515: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-17 11:25:37.883101: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-17 11:25:38.055131: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-17 11:25:39.360870: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-17 11:25:52.138526: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Cargar el dataset MNIST\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0ee2c4-1f24-46c6-ad07-47e2e44a9fed",
   "metadata": {},
   "source": [
    "**3. Preprocesamiento de Datos**\n",
    "\n",
    "Escalaremos las imágenes a valores entre 0 y 1 y convertiremos las etiquetas a una representación categórica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8d803f7-be0f-4d9a-9913-9ccae481fe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar las imágenes a valores entre 0 y 1\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Convertir las etiquetas a formato categórico\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, 10)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9af874-5143-49ea-b385-4287df9685ee",
   "metadata": {},
   "source": [
    "**4. Construcción del Modelo ANN**\n",
    "\n",
    "Aquí construiremos una red neuronal simple con una capa oculta:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e06adee-603a-47f1-8c9d-4b6f0f9f4843",
   "metadata": {},
   "source": [
    "El modelo de red neuronal artificial (ANN) es un ejemplo básico de una arquitectura **feedforward** con capas densas totalmente conectadas, diseñada para la clasificación de imágenes en 10 categorías (como el caso de la base de datos MNIST de dígitos escritos a mano). A continuación se presenta el **marco teórico** que sustenta esta arquitectura, acompañado de explicaciones y ecuaciones relacionadas.\n",
    "\n",
    "**4.1. Estructura General del Modelo**\n",
    "\n",
    "El modelo consta de tres capas principales:\n",
    "- **Capa de entrada (Flatten)**: Convierte una imagen de 28x28 píxeles en un vector de 784 elementos.\n",
    "- **Capa oculta (Dense)**: Tiene 128 neuronas y usa la función de activación **ReLU** (Rectified Linear Unit).\n",
    "- **Capa de salida (Dense)**: Tiene 10 neuronas, una por cada clase posible, con la función de activación **softmax** para obtener probabilidades de clasificación.\n",
    "\n",
    "**4.2. Capa de Entrada: `Flatten(input_shape=(28, 28))`**\n",
    "\n",
    "Esta capa simplemente reorganiza los datos de entrada. La imagen de 28x28 píxeles se convierte en un vector de longitud 784 (28 * 28). No tiene pesos ni funciones de activación.\n",
    "\n",
    "Sea **X** el tensor de entrada de la red, con una forma (28, 28). Tras la capa de flattening, se convierte en:\n",
    "\n",
    "$\n",
    "\\mathbf{x} \\in \\mathbb{R}^{784}\n",
    "$\n",
    "\n",
    "Este vector se utiliza como entrada para la capa densa siguiente.\n",
    "\n",
    "**4.3. Capa Oculta: `Dense(128, activation='relu')`**\n",
    "\n",
    "Esta es una capa densa con 128 neuronas. Cada neurona está completamente conectada a la entrada (el vector de 784 elementos). La operación básica en una neurona es una combinación lineal de los pesos y el sesgo (bias), seguido de la aplicación de una función de activación no lineal.\n",
    "\n",
    "La salida de una neurona $ j $ en esta capa oculta se calcula como:\n",
    "\n",
    "$\n",
    "z_j = \\sum_{i=1}^{784} w_{ji} x_i + b_j\n",
    "$\n",
    "\n",
    "Donde:\n",
    "- $ x_i $ son los valores de entrada (el vector aplanado),\n",
    "- $w_{ji}$ son los pesos asociados a cada conexión entre la entrada $i$ y la neurona $j$,\n",
    "- $b_j$ es el sesgo para la neurona $j$,\n",
    "- $z_j$ es la suma ponderada que llega a la neurona $j$.\n",
    "\n",
    "**Activación ReLU:**\n",
    "\n",
    "Después de calcular $z_j$, se aplica la función de activación **ReLU** (Rectified Linear Unit). ReLU se utiliza para introducir no linealidad en la red, lo que permite que la red neuronal aprenda relaciones complejas.\n",
    "\n",
    "La función **ReLU** se define como:\n",
    "\n",
    "$\n",
    "\\text{ReLU}(z_j) = \\max(0, z_j)\n",
    "$\n",
    "\n",
    "Esto significa que:\n",
    "- Si $ z_j \\geq 0 $, entonces $ \\text{ReLU}(z_j) = z_j $,\n",
    "- Si $ z_j < 0 $, entonces $ \\text{ReLU}(z_j) = 0 $.\n",
    "\n",
    "Esta función selecciona el valor máximo entre 0 y el valor de la suma ponderada $z_j$, descartando los valores negativos. Por ejemplo:\n",
    "- Si $ z_j = -5 $, entonces $ \\text{ReLU}(-5) = 0 $.\n",
    "- Si $ z_j = 3 $, entonces $ \\text{ReLU}(3) = 3 $.\n",
    "\n",
    "Esto permite que las neuronas solo se activen (es decir, transmitan una señal) cuando el valor $z_j$ sea positivo, lo que facilita un entrenamiento eficiente y una propagación adecuada de los gradientes durante la optimización.\n",
    "\n",
    "**4.4. Capa de Salida: `Dense(10, activation='softmax')`**\n",
    "Esta es una capa densa con 10 neuronas, una para cada clase. Su objetivo es calcular la probabilidad de que la entrada pertenezca a cada una de las 10 clases. Cada neurona de salida calcula una suma ponderada similar a la capa oculta, pero en lugar de la función ReLU, se utiliza la función de activación **softmax**.\n",
    "\n",
    "Para la salida $k$, la función **softmax** se define como:\n",
    "\n",
    "$\n",
    "\\hat{y}_k = \\frac{e^{z_k}}{\\sum_{j=1}^{10} e^{z_j}}\n",
    "$\n",
    "\n",
    "Donde:\n",
    "- $ \\hat{y}_k $ es la probabilidad predicha para la clase $k$,\n",
    "- $z_k$ es la entrada a la neurona $k$ de la capa de salida (la suma ponderada de la neurona $k$.\n",
    "\n",
    "La función **softmax** asegura que la salida de la red sea un vector de probabilidades, es decir, los valores estarán entre 0 y 1, y su suma será 1. Esto permite interpretar la salida como las probabilidades de que la entrada pertenezca a cada clase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "293b534e-0a09-459f-acd2-6aa3b32a3909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julio/.local/lib/python3.10/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Construir el modelo ANN\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),  # Convertir imágenes 28x28 a un vector de 784 elementos\n",
    "    tf.keras.layers.Dense(128, activation='relu'),  # Capa oculta con 128 neuronas y activación ReLU\n",
    "    tf.keras.layers.Dense(10, activation='softmax')  # Capa de salida con 10 neuronas (una por clase) y activación softmax\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66bcff6-4f06-4743-adfa-90b36e5cd89e",
   "metadata": {},
   "source": [
    "**5. Entrenamiento del Modelo**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aac774-bde2-4645-a6a2-d416cbe2f15e",
   "metadata": {},
   "source": [
    "**5.1. Función de Pérdida: Entropía Cruzada Categórica**\n",
    "\n",
    "Dado que se trata de un problema de clasificación multiclase, la función de pérdida más común es la **entropía cruzada categórica**, que mide la diferencia entre la distribución de probabilidades verdadera y la distribución predicha por el modelo. \n",
    "\n",
    "La entropía cruzada se define como:\n",
    "\n",
    "$\n",
    "\\mathcal{L} = - \\sum_{k=1}^{10} y_k \\log(\\hat{y}_k)\n",
    "$\n",
    "\n",
    "Donde:\n",
    "- $y_k$ es la etiqueta verdadera para la clase $k$  (1 si es la clase verdadera, 0 en caso contrario),\n",
    "- $\\hat{y}_k$ es la probabilidad predicha para la clase $k$ .\n",
    "\n",
    "**5.2. Entrenamiento y Optimización**\n",
    "\n",
    "Durante el entrenamiento, el modelo ajusta los pesos y sesgos mediante un algoritmo de optimización como **gradiente descendente** o variantes como **Adam**. El objetivo es minimizar la función de pérdida ajustando los pesos $w$ y los sesgos $b$.\n",
    "\n",
    "La actualización de los pesos sigue la dirección del gradiente negativo de la pérdida con respecto a los pesos:\n",
    "\n",
    "$\n",
    "w_{ji} \\leftarrow w_{ji} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial w_{ji}}\n",
    "$\n",
    "\n",
    "Donde $\\eta$ es la tasa de aprendizaje, y $\\frac{\\partial \\mathcal{L}}{\\partial w_{ji}}$ es el gradiente de la pérdida con respecto a los pesos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663d090a-1aad-4fcb-a942-5cee6bf77969",
   "metadata": {},
   "source": [
    "**Compilaremos y entrenaremos el modelo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01507206-0cd8-46ef-b521-350d2af1a117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 - 10s - 7ms/step - accuracy: 0.9190 - loss: 0.2866 - val_accuracy: 0.9557 - val_loss: 0.1525\n",
      "Epoch 2/5\n",
      "1500/1500 - 7s - 4ms/step - accuracy: 0.9628 - loss: 0.1289 - val_accuracy: 0.9638 - val_loss: 0.1220\n",
      "Epoch 3/5\n",
      "1500/1500 - 8s - 5ms/step - accuracy: 0.9743 - loss: 0.0870 - val_accuracy: 0.9712 - val_loss: 0.0989\n",
      "Epoch 4/5\n",
      "1500/1500 - 8s - 5ms/step - accuracy: 0.9811 - loss: 0.0644 - val_accuracy: 0.9730 - val_loss: 0.0916\n",
      "Epoch 5/5\n",
      "1500/1500 - 6s - 4ms/step - accuracy: 0.9846 - loss: 0.0499 - val_accuracy: 0.9747 - val_loss: 0.0887\n"
     ]
    }
   ],
   "source": [
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(train_images, train_labels, epochs=5, batch_size=32,\n",
    "                    validation_split=0.2,  # Usar el 20% de los datos de entrenamiento para validación\n",
    "                    verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1667b483-4a33-40b9-9149-a742b82da77d",
   "metadata": {},
   "source": [
    "**6. Evaluación del Modelo**\n",
    "\n",
    "Finalmente, evaluaremos el rendimiento del modelo en el conjunto de datos de prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f7fa416-e196-403a-b5e1-d62f2bb0ad5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - 3ms/step - accuracy: 0.9757 - loss: 0.0774\n",
      "\n",
      "Test accuracy: 0.9757000207901001\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f'\\nTest accuracy: {test_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
